---
layout: post
title: Dense Trajectory Notes
categories: 
- notes
- DTF
- Computer Vision
tags:
- Notes
- Computer Vision
status: publish
type: post
published: true
meta:
  _publicize_pending: '1'
  _wpas_facebook_publicize_failure: 'O:13:"Keyring_Error":2:{s:6:"errors";a:1:{s:21:"keyring-request-error";a:1:{i:0;a:5:{s:7:"headers";a:11:{s:27:"access-control-allow-origin";s:1:"*";s:13:"cache-control";s:8:"no-store";s:12:"content-type";s:31:"application/json;
    charset=UTF-8";s:7:"expires";s:29:"Sat, 01 Jan 2000 00:00:00 GMT";s:6:"pragma";s:8:"no-cache";s:16:"www-authenticate";s:150:"OAuth
    "Facebook Platform" "invalid_token" "Error validating access token: The session
    has been invalidated because the user has changed the password."";s:8:"x-fb-rev";s:7:"1076183";s:10:"x-fb-debug";s:44:"agza+xUEX+yiU6ho49l8NRpCJPZLG5T//dctkNPUZSE=";s:4:"date";s:29:"Fri,
    10 Jan 2014 21:44:28 GMT";s:10:"connection";s:5:"close";s:14:"content-length";s:3:"185";}s:4:"body";s:185:"{"error":{"message":"Error
    validating access token: The session has been invalidated because the user has
    changed the password.","type":"OAuthException","code":190,"error_subcode":460}}";s:8:"response";a:2:{s:4:"code";i:401;s:7:"message";s:12:"Unauthorized";}s:7:"cookies";a:0:{}s:8:"filename";N;}}}s:10:"error_data";a:0:{}}'
  publicize_facebook_url: https://facebook.com/
  publicize_linkedin_url: http://www.linkedin.com/updates?discuss=&scope=74863407&stype=M&topic=5827525162882314240&type=U&a=0g1t
  _wpas_done_373263: '1'
  _publicize_done_external: a:1:{s:8:"linkedin";a:1:{s:10:"DFfbe-Shd5";b:1;}}
  _wpas_skip_373264: '1'
  _wpas_skip_373263: '1'
author: 
---
<p><span style="font-size:12px;font-weight:bold;line-height:1em;">Notes of Dense Trajectories</span><img class="aligncenter" style="font-size:12px;font-weight:bold;line-height:1em;" alt="" src="https://lh6.googleusercontent.com/AysSQ7M6yvc5hbkBHzsEHc1c7L4ZJcIXmdSfzh9g4iBDsL_q9lZD_8fhH8TEPMwAJ12Kp_s4z1FWmX3rndc6DTFAwARyWkx-mJiixB4X7nQRV5XcB391xltefQ" width="568px;" height="178px;" /></p>
<ul>
<li>densely sample feature points in each frame</li>
<li>track points in the video based on optical flow.</li>
<li>compute multiple descriptors along the trajectories of feature points to capture shape, appearance and motion information.</li>
</ul>
<ul>
<li>
<h3>Dense Sampling</h3>
<ul>
<li>Sampling step size \( W=5 \) pixels</li>
<li># spatial scales ≤ 8</li>
<li>Spatial scale increase: \( 1 / \sqrt{2} \)</li>
<li>Removing points in homogeneous areas: 
$$ T=0.001 \times \max_{i \in l}\min(\lambda_{i}^{1},\lambda_{i}^{2}) $$,
where \( (\lambda_{i}^{1},\lambda_{i}^{2}) \) are eigenvalues of point \(i\) in image \(I\) (the auto-correlation matrix).</span></li>
</ul>
</li>
</ul>
<ul>
<li>
<h3>Descriptors</h3>
<ul>
<li>
<p dir="ltr">Trajectory shape descriptor(TR):</p>
</li>
</ul>
</li>
</ul>
<p dir="ltr"><img alt="" src="https://lh3.googleusercontent.com/TZG6Zx9WlI95UsEDsemdeXUHBtPrV6gJh_50zZzKJYTV45heKGfIx7xjlN079nJUZWMb93j9Vr72R0cvPQtdyVb49vDDeFpcLhcjeVq_VL8JhSh2VlZwtvNnxw" width="206px;" height="61px;" /></p>
<p dir="ltr">where L is the length of trajectory, and the displacement vectors <img alt="" src="https://lh3.googleusercontent.com/DqC7Y6vrH4mfXOVQQ29w8S9KH4jNH6kPpmyViWd5AhXhpVebHPO5xMTIdyWVAOPMvFU379KWsBR5FjV6cER0-whR6TxlBpch33vMqEVDXwC80uon5hQJMR3cxg" width="422px;" height="27px;" /></p>
<ul>
<li>HOG – static appearance information</li>
<li>HOF – local motion information</li>
<li>MBH – motion descriptor for trajectories</li>
</ul>
<ul>
<li>
<h3>Format of DTF features</h3>
</li>
</ul>
<h4>The format of the computed features</h4>
<p dir="ltr">The features are computed one by one, and each one in a single line, with the following format:</p>
<p dir="ltr">frameNum mean_x mean_y var_x var_y length scale x_pos y_pos t_pos Trajectory HOG HOF MBHx MBHy</p>
<p dir="ltr">The first 10 elements are information about the trajectory:</p>
<ul>
<li>frameNum:     The trajectory ends on which frame</li>
<li>mean_x:       The mean value of the x coordinates of the trajectory</li>
<li>mean_y:       The mean value of the y coordinates of the trajectory</li>
<li>var_x:        The variance of the x coordinates of the trajectory</li>
<li>var_y:        The variance of the y coordinates of the trajectory</li>
<li>length:       The length of the trajectory</li>
<li>scale:        The trajectory is computed on which scale</li>
<li>x_pos:        The normalized x position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
<li>y_pos:        The normalized y position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
<li>t_pos:        The normalized t position w.r.t. the video (0~0.999), for spatio-temporal pyramid</li>
</ul>
<p dir="ltr">The following element are five descriptors concatenated one by one:</p>
<ul>
<li>Trajectory:    2x[trajectory length] (default 30 dimension)</li>
<li>HOG:           8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
<li>HOF:           9x[spatial cells]x[spatial cells]x[temporal cells] (default 108 dimension)</li>
<li>MBHx:          8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
<li>MBHy:          8x[spatial cells]x[spatial cells]x[temporal cells] (default 96 dimension)</li>
</ul>
<ol start="2">
<li>
<h3>Improved Dense Trajectories</h3>
</li>
</ol>
<ul>
<li>Explicit camera motion estimation</li>
<li>Assumption: two consecutive frames are related by a homography.</li>
<li>Match feature points between frames using SURF descriptors and dense optical flow</li>
<li>Removing inconsistent matches due to humans: use a human detector to remove matches from human regions (computation expensive)</li>
<li>Estimate a homography with RANSAC with these matches</li>
</ul>
<h3>References:</h3>
<ol>
<li>
<div>H Wang, C Schmid, Action recognition with improved trajectories, ICCV 2013</div>
</li>
<li>
<div>H Wang, A Kläser, C Schmid, CL Liu, Dense trajectories and motion boundary descriptors for action recognition, International Journal of Computer Vision, May 2013, Volume 103, Issue 1, pp 60-79</div>
</li>
</ol>
